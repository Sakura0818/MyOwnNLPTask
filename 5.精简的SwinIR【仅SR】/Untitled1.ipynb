{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c49b560",
   "metadata": {},
   "outputs": [],
   "source": [
    "myopt={\n",
    "  \"task\":\"swinir_sr_lightweight_x2\"     \n",
    "  , \"model\": \"plain\" \n",
    "  , \"gpu_ids\": [0]\n",
    "  , \"dist\": False\n",
    "\n",
    "  , \"scale\": 2       \n",
    "  , \"n_channels\": 3  \n",
    "\n",
    "  , \"path\": {\n",
    "    \"root\": \"superresolution\"            \n",
    "    , \"pretrained_netG\": None      \n",
    "    , \"pretrained_netE\": None      \n",
    "  }\n",
    "\n",
    "  , \"datasets\": {\n",
    "    \"train\": {\n",
    "      \"name\": \"train_dataset\"          \n",
    "      , \"dataset_type\": \"sr\"         \n",
    "      , \"dataroot_H\": \"trainsets/trainH\"\n",
    "      , \"dataroot_L\": \"trainsets/trainL\"  \n",
    "\n",
    "      , \"H_size\": 128                   \n",
    "\n",
    "      , \"dataloader_shuffle\": True\n",
    "      , \"dataloader_num_workers\": 0\n",
    "      , \"dataloader_batch_size\": 4      \n",
    "    }\n",
    "    , \"test\": {\n",
    "      \"name\": \"test_dataset\"            \n",
    "      , \"dataset_type\": \"sr\"         \n",
    "      , \"dataroot_H\": \"testsets/Set5/HR\"  \n",
    "      , \"dataroot_L\": \"testsets/Set5/LR_bicubic/X2\"   \n",
    "\n",
    "    }\n",
    "  }\n",
    "\n",
    "  , \"netG\": {\n",
    "    \"net_type\": \"swinir\" \n",
    "    , \"upscale\": 2                      \n",
    "    , \"in_chans\": 3 \n",
    "    , \"img_size\": 64\n",
    "    , \"window_size\": 8  \n",
    "    , \"img_range\": 1.0 \n",
    "    , \"depths\": [6, 6, 6, 6]\n",
    "    , \"embed_dim\": 60 \n",
    "    , \"num_heads\": [6, 6, 6, 6]\n",
    "    , \"mlp_ratio\": 2 \n",
    "    , \"upsampler\": \"pixelshuffledirect\"       \n",
    "    , \"resi_connection\": \"1conv\"       \n",
    "\n",
    "    , \"init_type\": \"default\"\n",
    "  }\n",
    "\n",
    "  , \"train\": {\n",
    "    \"G_lossfn_type\": \"l1\"               \n",
    "    , \"G_lossfn_weight\": 1.0           \n",
    "\n",
    "    , \"E_decay\": 0.999                  \n",
    "\n",
    "    , \"G_optimizer_type\": \"adam\"       \n",
    "    , \"G_optimizer_lr\": 2e-4            \n",
    "    , \"G_optimizer_wd\": 0              \n",
    "    , \"G_optimizer_clipgrad\": None      \n",
    "    , \"G_optimizer_reuse\": True         \n",
    "\n",
    "    , \"G_scheduler_type\": \"MultiStepLR\" \n",
    "    , \"G_scheduler_milestones\": [250000, 400000, 450000, 475000, 500000]\n",
    "    , \"G_scheduler_gamma\": 0.5\n",
    "\n",
    "    , \"G_regularizer_orthstep\": None    \n",
    "    , \"G_regularizer_clipstep\": None    \n",
    "\n",
    "    , \"G_param_strict\": True\n",
    "    , \"E_param_strict\": True\n",
    "\n",
    "    , \"checkpoint_test\":  4          \n",
    "    , \"checkpoint_save\":  4          \n",
    "    , \"checkpoint_print\": 2           \n",
    "  }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "313181a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import math\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import logging\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch\n",
    "\n",
    "from utils import utils_logger\n",
    "from utils import utils_image as util\n",
    "from utils import utils_option as option\n",
    "from utils.utils_dist import get_dist_info, init_dist\n",
    "\n",
    "from data.select_dataset import define_Dataset\n",
    "from models.select_model import define_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5315f642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61110e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75fb13f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path='options/train_swinir_sr_lightweight.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "091d13f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export CUDA_VISIBLE_DEVICES=0\n",
      "number of GPUs is: 1\n"
     ]
    }
   ],
   "source": [
    "opt = option.parse(json_path, is_train=True)\n",
    "opt['dist'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "712731dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('task', 'swinir_sr_lightweight_x2'),\n",
       "             ('model', 'plain'),\n",
       "             ('gpu_ids', [0]),\n",
       "             ('dist', False),\n",
       "             ('scale', 2),\n",
       "             ('n_channels', 3),\n",
       "             ('path',\n",
       "              OrderedDict([('root', 'superresolution'),\n",
       "                           ('pretrained_netG', None),\n",
       "                           ('pretrained_netE', None),\n",
       "                           ('task',\n",
       "                            'superresolution\\\\swinir_sr_lightweight_x2'),\n",
       "                           ('log',\n",
       "                            'superresolution\\\\swinir_sr_lightweight_x2'),\n",
       "                           ('options',\n",
       "                            'superresolution\\\\swinir_sr_lightweight_x2\\\\options'),\n",
       "                           ('models',\n",
       "                            'superresolution\\\\swinir_sr_lightweight_x2\\\\models'),\n",
       "                           ('images',\n",
       "                            'superresolution\\\\swinir_sr_lightweight_x2\\\\images')])),\n",
       "             ('datasets',\n",
       "              OrderedDict([('train',\n",
       "                            OrderedDict([('name', 'train_dataset'),\n",
       "                                         ('dataset_type', 'sr'),\n",
       "                                         ('dataroot_H', 'trainsets/trainH'),\n",
       "                                         ('dataroot_L', 'trainsets/trainL'),\n",
       "                                         ('H_size', 128),\n",
       "                                         ('dataloader_shuffle', True),\n",
       "                                         ('dataloader_num_workers', 0),\n",
       "                                         ('dataloader_batch_size', 4),\n",
       "                                         ('phase', 'train'),\n",
       "                                         ('scale', 2),\n",
       "                                         ('n_channels', 3)])),\n",
       "                           ('test',\n",
       "                            OrderedDict([('name', 'test_dataset'),\n",
       "                                         ('dataset_type', 'sr'),\n",
       "                                         ('dataroot_H', 'testsets/Set5/HR'),\n",
       "                                         ('dataroot_L',\n",
       "                                          'testsets/Set5/LR_bicubic/X2'),\n",
       "                                         ('phase', 'test'),\n",
       "                                         ('scale', 2),\n",
       "                                         ('n_channels', 3)]))])),\n",
       "             ('netG',\n",
       "              OrderedDict([('net_type', 'swinir'),\n",
       "                           ('upscale', 2),\n",
       "                           ('in_chans', 3),\n",
       "                           ('img_size', 64),\n",
       "                           ('window_size', 8),\n",
       "                           ('img_range', 1.0),\n",
       "                           ('depths', [6, 6, 6, 6]),\n",
       "                           ('embed_dim', 60),\n",
       "                           ('num_heads', [6, 6, 6, 6]),\n",
       "                           ('mlp_ratio', 2),\n",
       "                           ('upsampler', 'pixelshuffledirect'),\n",
       "                           ('resi_connection', '1conv'),\n",
       "                           ('init_type', 'default'),\n",
       "                           ('scale', 2)])),\n",
       "             ('train',\n",
       "              OrderedDict([('G_lossfn_type', 'l1'),\n",
       "                           ('G_lossfn_weight', 1.0),\n",
       "                           ('E_decay', 0.999),\n",
       "                           ('G_optimizer_type', 'adam'),\n",
       "                           ('G_optimizer_lr', 0.0002),\n",
       "                           ('G_optimizer_wd', 0),\n",
       "                           ('G_optimizer_clipgrad', None),\n",
       "                           ('G_optimizer_reuse', True),\n",
       "                           ('G_scheduler_type', 'MultiStepLR'),\n",
       "                           ('G_scheduler_milestones',\n",
       "                            [250000, 400000, 450000, 475000, 500000]),\n",
       "                           ('G_scheduler_gamma', 0.5),\n",
       "                           ('G_regularizer_orthstep', None),\n",
       "                           ('G_regularizer_clipstep', None),\n",
       "                           ('G_param_strict', True),\n",
       "                           ('E_param_strict', True),\n",
       "                           ('checkpoint_test', 4),\n",
       "                           ('checkpoint_save', 4),\n",
       "                           ('checkpoint_print', 2),\n",
       "                           ('F_feature_layer', 34),\n",
       "                           ('F_weights', 1.0),\n",
       "                           ('F_lossfn_type', 'l1'),\n",
       "                           ('F_use_input_norm', True),\n",
       "                           ('F_use_range_norm', False),\n",
       "                           ('G_optimizer_betas', [0.9, 0.999]),\n",
       "                           ('G_scheduler_restart_weights', 1)])),\n",
       "             ('opt_path', 'options/train_swinir_sr_lightweight.json'),\n",
       "             ('is_train', True),\n",
       "             ('merge_bn', False),\n",
       "             ('merge_bn_startpoint', -1),\n",
       "             ('find_unused_parameters', True),\n",
       "             ('use_static_graph', False),\n",
       "             ('num_gpu', 1)])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f692a40d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-12-11 15:12:06.684 :   task: swinir_sr_lightweight_x2\n",
      "  model: plain\n",
      "  gpu_ids: [0]\n",
      "  dist: False\n",
      "  scale: 2\n",
      "  n_channels: 3\n",
      "  path:[\n",
      "    root: superresolution\n",
      "    pretrained_netG: superresolution\\swinir_sr_lightweight_x2\\models\\260_G.pth\n",
      "    pretrained_netE: superresolution\\swinir_sr_lightweight_x2\\models\\260_E.pth\n",
      "    task: superresolution\\swinir_sr_lightweight_x2\n",
      "    log: superresolution\\swinir_sr_lightweight_x2\n",
      "    options: superresolution\\swinir_sr_lightweight_x2\\options\n",
      "    models: superresolution\\swinir_sr_lightweight_x2\\models\n",
      "    images: superresolution\\swinir_sr_lightweight_x2\\images\n",
      "    pretrained_optimizerG: superresolution\\swinir_sr_lightweight_x2\\models\\260_optimizerG.pth\n",
      "  ]\n",
      "  datasets:[\n",
      "    train:[\n",
      "      name: train_dataset\n",
      "      dataset_type: sr\n",
      "      dataroot_H: trainsets/trainH\n",
      "      dataroot_L: trainsets/trainL\n",
      "      H_size: 128\n",
      "      dataloader_shuffle: True\n",
      "      dataloader_num_workers: 0\n",
      "      dataloader_batch_size: 4\n",
      "      phase: train\n",
      "      scale: 2\n",
      "      n_channels: 3\n",
      "    ]\n",
      "    test:[\n",
      "      name: test_dataset\n",
      "      dataset_type: sr\n",
      "      dataroot_H: testsets/Set5/HR\n",
      "      dataroot_L: testsets/Set5/LR_bicubic/X2\n",
      "      phase: test\n",
      "      scale: 2\n",
      "      n_channels: 3\n",
      "    ]\n",
      "  ]\n",
      "  netG:[\n",
      "    net_type: swinir\n",
      "    upscale: 2\n",
      "    in_chans: 3\n",
      "    img_size: 64\n",
      "    window_size: 8\n",
      "    img_range: 1.0\n",
      "    depths: [6, 6, 6, 6]\n",
      "    embed_dim: 60\n",
      "    num_heads: [6, 6, 6, 6]\n",
      "    mlp_ratio: 2\n",
      "    upsampler: pixelshuffledirect\n",
      "    resi_connection: 1conv\n",
      "    init_type: default\n",
      "    scale: 2\n",
      "  ]\n",
      "  train:[\n",
      "    G_lossfn_type: l1\n",
      "    G_lossfn_weight: 1.0\n",
      "    E_decay: 0.999\n",
      "    G_optimizer_type: adam\n",
      "    G_optimizer_lr: 0.0002\n",
      "    G_optimizer_wd: 0\n",
      "    G_optimizer_clipgrad: None\n",
      "    G_optimizer_reuse: True\n",
      "    G_scheduler_type: MultiStepLR\n",
      "    G_scheduler_milestones: [250000, 400000, 450000, 475000, 500000]\n",
      "    G_scheduler_gamma: 0.5\n",
      "    G_regularizer_orthstep: None\n",
      "    G_regularizer_clipstep: None\n",
      "    G_param_strict: True\n",
      "    E_param_strict: True\n",
      "    checkpoint_test: 4\n",
      "    checkpoint_save: 4\n",
      "    checkpoint_print: 2\n",
      "    F_feature_layer: 34\n",
      "    F_weights: 1.0\n",
      "    F_lossfn_type: l1\n",
      "    F_use_input_norm: True\n",
      "    F_use_range_norm: False\n",
      "    G_optimizer_betas: [0.9, 0.999]\n",
      "    G_scheduler_restart_weights: 1\n",
      "  ]\n",
      "  opt_path: options/train_swinir_sr_lightweight.json\n",
      "  is_train: True\n",
      "  merge_bn: False\n",
      "  merge_bn_startpoint: -1\n",
      "  find_unused_parameters: True\n",
      "  use_static_graph: False\n",
      "  num_gpu: 1\n",
      "  rank: 0\n",
      "  world_size: 1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogHandlers exist!\n",
      "Random seed: 9378\n"
     ]
    }
   ],
   "source": [
    "opt['rank'], opt['world_size'] = get_dist_info()\n",
    "\n",
    "if opt['rank'] == 0:\n",
    "    util.mkdirs((path for key, path in opt['path'].items() if 'pretrained' not in key))\n",
    "\n",
    "# update opt\n",
    "\n",
    "# -->-->-->-->-->-->-->-->-->-->-->-->-->-\n",
    "init_iter_G, init_path_G = option.find_last_checkpoint(opt['path']['models'], net_type='G')\n",
    "init_iter_E, init_path_E = option.find_last_checkpoint(opt['path']['models'], net_type='E')\n",
    "opt['path']['pretrained_netG'] = init_path_G\n",
    "opt['path']['pretrained_netE'] = init_path_E\n",
    "init_iter_optimizerG, init_path_optimizerG = option.find_last_checkpoint(opt['path']['models'], net_type='optimizerG')\n",
    "opt['path']['pretrained_optimizerG'] = init_path_optimizerG\n",
    "current_step = max(init_iter_G, init_iter_E, init_iter_optimizerG)\n",
    "\n",
    "border = opt['scale']\n",
    "# --<--<--<--<--<--<--<--<--<--<--<--<--<-\n",
    "# ----------------------------------------\n",
    "# save opt to  a '../option.json' file\n",
    "# ----------------------------------------\n",
    "if opt['rank'] == 0:\n",
    "    option.save(opt)\n",
    "\n",
    "# ----------------------------------------\n",
    "\n",
    "# ----------------------------------------\n",
    "# return None for missing key\n",
    "# ----------------------------------------\n",
    "opt = option.dict_to_nonedict(opt)\n",
    "\n",
    "# ----------------------------------------\n",
    "# configure logger\n",
    "# ----------------------------------------\n",
    "if opt['rank'] == 0:\n",
    "    logger_name = 'train'\n",
    "    utils_logger.logger_info(logger_name, os.path.join(opt['path']['log'], logger_name+'.log'))\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    logger.info(option.dict2str(opt))\n",
    "\n",
    "# ----------------------------------------\n",
    "# seed\n",
    "# ----------------------------------------\n",
    "seed = opt['train']['manual_seed']\n",
    "if seed is None:\n",
    "    seed = random.randint(1, 10000)\n",
    "print('Random seed: {}'.format(seed))\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "142e8a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-12-11 15:12:42.197 : Number of train images: 10, iters: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset [DatasetSR - train_dataset] is created.\n",
      "Dataset [DatasetSR - test_dataset] is created.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    '''\n",
    "    # ----------------------------------------\n",
    "    # Step--2 (creat dataloader)\n",
    "    # ----------------------------------------\n",
    "    '''\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 1) create_dataset\n",
    "    # 2) creat_dataloader for train and test\n",
    "    # ----------------------------------------\n",
    "    for phase, dataset_opt in opt['datasets'].items():\n",
    "        if phase == 'train':\n",
    "            train_set = define_Dataset(dataset_opt)\n",
    "            train_size = int(math.ceil(len(train_set) / dataset_opt['dataloader_batch_size']))\n",
    "            if opt['rank'] == 0:\n",
    "                logger.info('Number of train images: {:,d}, iters: {:,d}'.format(len(train_set), train_size))\n",
    "            if opt['dist']:\n",
    "                train_sampler = DistributedSampler(train_set, shuffle=dataset_opt['dataloader_shuffle'], drop_last=True, seed=seed)\n",
    "                train_loader = DataLoader(train_set,\n",
    "                                          batch_size=dataset_opt['dataloader_batch_size']//opt['num_gpu'],\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=dataset_opt['dataloader_num_workers']//opt['num_gpu'],\n",
    "                                          drop_last=True,\n",
    "                                          pin_memory=True,\n",
    "                                          sampler=train_sampler)\n",
    "            else:\n",
    "                train_loader = DataLoader(train_set,\n",
    "                                          batch_size=dataset_opt['dataloader_batch_size'],\n",
    "                                          shuffle=dataset_opt['dataloader_shuffle'],\n",
    "                                          num_workers=dataset_opt['dataloader_num_workers'],\n",
    "                                          drop_last=True,\n",
    "                                          pin_memory=True)\n",
    "\n",
    "        elif phase == 'test':\n",
    "            test_set = define_Dataset(dataset_opt)\n",
    "            test_loader = DataLoader(test_set, batch_size=1,\n",
    "                                     shuffle=False, num_workers=1,\n",
    "                                     drop_last=False, pin_memory=True)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Phase [%s] is not recognized.\" % phase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0334992",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda\\envs\\swinir\\lib\\site-packages\\torch\\functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass this initialization! Initialization was done during network definition!\n",
      "Pass this initialization! Initialization was done during network definition!\n",
      "Training model [ModelPlain] is created.\n",
      "Loading model for G [superresolution\\swinir_sr_lightweight_x2\\models\\260_G.pth] ...\n",
      "Loading model for E [superresolution\\swinir_sr_lightweight_x2\\models\\260_E.pth] ...\n",
      "Loading optimizerG [superresolution\\swinir_sr_lightweight_x2\\models\\260_optimizerG.pth] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-12-11 15:13:09.785 : \n",
      "Networks name: SwinIR\n",
      "Params number: 910152\n",
      "Net structure:\n",
      "SwinIR(\n",
      "  (conv_first): Conv2d(3, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (norm): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (patch_unembed): PatchUnEmbed()\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0): RSTB(\n",
      "      (residual_group): BasicLayer(\n",
      "        dim=60, input_resolution=(64, 64), depth=6\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.004)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.009)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.013)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.017)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.022)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (patch_embed): PatchEmbed()\n",
      "      (patch_unembed): PatchUnEmbed()\n",
      "    )\n",
      "    (1): RSTB(\n",
      "      (residual_group): BasicLayer(\n",
      "        dim=60, input_resolution=(64, 64), depth=6\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.026)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.030)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.035)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.039)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.043)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.048)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (patch_embed): PatchEmbed()\n",
      "      (patch_unembed): PatchUnEmbed()\n",
      "    )\n",
      "    (2): RSTB(\n",
      "      (residual_group): BasicLayer(\n",
      "        dim=60, input_resolution=(64, 64), depth=6\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.052)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.057)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.061)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.065)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.070)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.074)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (patch_embed): PatchEmbed()\n",
      "      (patch_unembed): PatchUnEmbed()\n",
      "    )\n",
      "    (3): RSTB(\n",
      "      (residual_group): BasicLayer(\n",
      "        dim=60, input_resolution=(64, 64), depth=6\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.078)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.083)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.087)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.091)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.096)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): SwinTransformerBlock(\n",
      "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
      "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=60, window_size=(8, 8), num_heads=6\n",
      "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.100)\n",
      "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (patch_embed): PatchEmbed()\n",
      "      (patch_unembed): PatchUnEmbed()\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
      "  (conv_after_body): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (upsample): UpsampleOneStep(\n",
      "    (0): Conv2d(60, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): PixelShuffle(upscale_factor=2)\n",
      "  )\n",
      ")\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-12-11 15:13:09.942 : \n",
      " |  mean  |  min   |  max   |  std   || shape               \n",
      " | -0.000 | -0.198 |  0.196 |  0.111 | torch.Size([60, 3, 3, 3]) || conv_first.weight\n",
      " | -0.029 | -0.189 |  0.186 |  0.107 | torch.Size([60]) || conv_first.bias\n",
      " |  1.001 |  0.996 |  1.007 |  0.002 | torch.Size([60]) || patch_embed.norm.weight\n",
      " |  0.000 | -0.004 |  0.003 |  0.001 | torch.Size([60]) || patch_embed.norm.bias\n",
      " |  0.999 |  0.996 |  1.004 |  0.002 | torch.Size([60]) || layers.0.residual_group.blocks.0.norm1.weight\n",
      " | -0.000 | -0.005 |  0.004 |  0.002 | torch.Size([60]) || layers.0.residual_group.blocks.0.norm1.bias\n",
      " | -0.001 | -0.071 |  0.071 |  0.021 | torch.Size([225, 6]) || layers.0.residual_group.blocks.0.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.0.attn.relative_position_index\n",
      " |  0.000 | -0.083 |  0.074 |  0.020 | torch.Size([180, 60]) || layers.0.residual_group.blocks.0.attn.qkv.weight\n",
      " |  0.000 | -0.010 |  0.007 |  0.002 | torch.Size([180]) || layers.0.residual_group.blocks.0.attn.qkv.bias\n",
      " | -0.001 | -0.063 |  0.066 |  0.020 | torch.Size([60, 60]) || layers.0.residual_group.blocks.0.attn.proj.weight\n",
      " |  0.000 | -0.004 |  0.005 |  0.002 | torch.Size([60]) || layers.0.residual_group.blocks.0.attn.proj.bias\n",
      " |  1.000 |  0.993 |  1.006 |  0.002 | torch.Size([60]) || layers.0.residual_group.blocks.0.norm2.weight\n",
      " | -0.000 | -0.005 |  0.005 |  0.002 | torch.Size([60]) || layers.0.residual_group.blocks.0.norm2.bias\n",
      " | -0.000 | -0.080 |  0.073 |  0.020 | torch.Size([120, 60]) || layers.0.residual_group.blocks.0.mlp.fc1.weight\n",
      " |  0.000 | -0.006 |  0.005 |  0.002 | torch.Size([120]) || layers.0.residual_group.blocks.0.mlp.fc1.bias\n",
      " | -0.000 | -0.081 |  0.066 |  0.020 | torch.Size([60, 120]) || layers.0.residual_group.blocks.0.mlp.fc2.weight\n",
      " |  0.000 | -0.004 |  0.005 |  0.002 | torch.Size([60]) || layers.0.residual_group.blocks.0.mlp.fc2.bias\n",
      " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.0.residual_group.blocks.1.attn_mask\n",
      " |  0.999 |  0.996 |  1.003 |  0.002 | torch.Size([60]) || layers.0.residual_group.blocks.1.norm1.weight\n",
      " |  0.000 | -0.005 |  0.005 |  0.002 | torch.Size([60]) || layers.0.residual_group.blocks.1.norm1.bias\n",
      " | -0.002 | -0.068 |  0.065 |  0.022 | torch.Size([225, 6]) || layers.0.residual_group.blocks.1.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.1.attn.relative_position_index\n",
      " | -0.000 | -0.075 |  0.070 |  0.021 | torch.Size([180, 60]) || layers.0.residual_group.blocks.1.attn.qkv.weight\n",
      " | -0.000 | -0.011 |  0.009 |  0.003 | torch.Size([180]) || layers.0.residual_group.blocks.1.attn.qkv.bias\n",
      " |  0.000 | -0.079 |  0.070 |  0.020 | torch.Size([60, 60]) || layers.0.residual_group.blocks.1.attn.proj.weight\n",
      " |  0.000 | -0.004 |  0.004 |  0.002 | torch.Size([60]) || layers.0.residual_group.blocks.1.attn.proj.bias\n",
      " |  1.000 |  0.994 |  1.006 |  0.003 | torch.Size([60]) || layers.0.residual_group.blocks.1.norm2.weight\n",
      " | -0.000 | -0.005 |  0.005 |  0.002 | torch.Size([60]) || layers.0.residual_group.blocks.1.norm2.bias\n",
      " | -0.000 | -0.080 |  0.077 |  0.020 | torch.Size([120, 60]) || layers.0.residual_group.blocks.1.mlp.fc1.weight\n",
      " |  0.000 | -0.004 |  0.006 |  0.002 | torch.Size([120]) || layers.0.residual_group.blocks.1.mlp.fc1.bias\n",
      " |  0.000 | -0.081 |  0.073 |  0.020 | torch.Size([60, 120]) || layers.0.residual_group.blocks.1.mlp.fc2.weight\n",
      " |  0.000 | -0.004 |  0.004 |  0.002 | torch.Size([60]) || layers.0.residual_group.blocks.1.mlp.fc2.bias\n",
      " |  0.998 |  0.995 |  1.003 |  0.002 | torch.Size([60]) || layers.0.residual_group.blocks.2.norm1.weight\n",
      " |  0.000 | -0.006 |  0.006 |  0.003 | torch.Size([60]) || layers.0.residual_group.blocks.2.norm1.bias\n",
      " | -0.001 | -0.067 |  0.058 |  0.022 | torch.Size([225, 6]) || layers.0.residual_group.blocks.2.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.2.attn.relative_position_index\n",
      " | -0.000 | -0.081 |  0.083 |  0.020 | torch.Size([180, 60]) || layers.0.residual_group.blocks.2.attn.qkv.weight\n",
      " | -0.000 | -0.010 |  0.008 |  0.003 | torch.Size([180]) || layers.0.residual_group.blocks.2.attn.qkv.bias\n",
      " | -0.000 | -0.071 |  0.066 |  0.020 | torch.Size([60, 60]) || layers.0.residual_group.blocks.2.attn.proj.weight\n",
      " |  0.000 | -0.004 |  0.005 |  0.002 | torch.Size([60]) || layers.0.residual_group.blocks.2.attn.proj.bias\n",
      " |  1.000 |  0.995 |  1.008 |  0.003 | torch.Size([60]) || layers.0.residual_group.blocks.2.norm2.weight\n",
      " | -0.000 | -0.007 |  0.006 |  0.003 | torch.Size([60]) || layers.0.residual_group.blocks.2.norm2.bias\n",
      " | -0.000 | -0.070 |  0.075 |  0.020 | torch.Size([120, 60]) || layers.0.residual_group.blocks.2.mlp.fc1.weight\n",
      " |  0.000 | -0.004 |  0.005 |  0.002 | torch.Size([120]) || layers.0.residual_group.blocks.2.mlp.fc1.bias\n",
      " | -0.000 | -0.076 |  0.081 |  0.020 | torch.Size([60, 120]) || layers.0.residual_group.blocks.2.mlp.fc2.weight\n",
      " |  0.000 | -0.004 |  0.005 |  0.002 | torch.Size([60]) || layers.0.residual_group.blocks.2.mlp.fc2.bias\n",
      " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.0.residual_group.blocks.3.attn_mask\n",
      " |  0.999 |  0.995 |  1.003 |  0.002 | torch.Size([60]) || layers.0.residual_group.blocks.3.norm1.weight\n",
      " |  0.000 | -0.006 |  0.005 |  0.003 | torch.Size([60]) || layers.0.residual_group.blocks.3.norm1.bias\n",
      " | -0.003 | -0.078 |  0.061 |  0.022 | torch.Size([225, 6]) || layers.0.residual_group.blocks.3.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.3.attn.relative_position_index\n",
      " |  0.000 | -0.079 |  0.087 |  0.020 | torch.Size([180, 60]) || layers.0.residual_group.blocks.3.attn.qkv.weight\n",
      " | -0.000 | -0.010 |  0.011 |  0.003 | torch.Size([180]) || layers.0.residual_group.blocks.3.attn.qkv.bias\n",
      " |  0.000 | -0.068 |  0.073 |  0.020 | torch.Size([60, 60]) || layers.0.residual_group.blocks.3.attn.proj.weight\n",
      " | -0.000 | -0.005 |  0.005 |  0.002 | torch.Size([60]) || layers.0.residual_group.blocks.3.attn.proj.bias\n",
      " |  1.001 |  0.997 |  1.011 |  0.003 | torch.Size([60]) || layers.0.residual_group.blocks.3.norm2.weight\n",
      " |  0.000 | -0.006 |  0.005 |  0.002 | torch.Size([60]) || layers.0.residual_group.blocks.3.norm2.bias\n",
      " | -0.000 | -0.075 |  0.074 |  0.020 | torch.Size([120, 60]) || layers.0.residual_group.blocks.3.mlp.fc1.weight\n",
      " |  0.000 | -0.004 |  0.006 |  0.002 | torch.Size([120]) || layers.0.residual_group.blocks.3.mlp.fc1.bias\n",
      " | -0.000 | -0.067 |  0.077 |  0.020 | torch.Size([60, 120]) || layers.0.residual_group.blocks.3.mlp.fc2.weight\n",
      " | -0.000 | -0.004 |  0.005 |  0.002 | torch.Size([60]) || layers.0.residual_group.blocks.3.mlp.fc2.bias\n",
      " |  0.999 |  0.995 |  1.003 |  0.002 | torch.Size([60]) || layers.0.residual_group.blocks.4.norm1.weight\n",
      " | -0.000 | -0.004 |  0.004 |  0.002 | torch.Size([60]) || layers.0.residual_group.blocks.4.norm1.bias\n",
      " | -0.002 | -0.082 |  0.088 |  0.021 | torch.Size([225, 6]) || layers.0.residual_group.blocks.4.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.4.attn.relative_position_index\n",
      " |  0.000 | -0.083 |  0.079 |  0.020 | torch.Size([180, 60]) || layers.0.residual_group.blocks.4.attn.qkv.weight\n",
      " | -0.000 | -0.008 |  0.008 |  0.002 | torch.Size([180]) || layers.0.residual_group.blocks.4.attn.qkv.bias\n",
      " | -0.000 | -0.063 |  0.074 |  0.020 | torch.Size([60, 60]) || layers.0.residual_group.blocks.4.attn.proj.weight\n",
      " | -0.000 | -0.004 |  0.005 |  0.002 | torch.Size([60]) || layers.0.residual_group.blocks.4.attn.proj.bias\n",
      " |  1.000 |  0.997 |  1.008 |  0.003 | torch.Size([60]) || layers.0.residual_group.blocks.4.norm2.weight\n",
      " |  0.000 | -0.005 |  0.004 |  0.002 | torch.Size([60]) || layers.0.residual_group.blocks.4.norm2.bias\n",
      " | -0.000 | -0.078 |  0.076 |  0.020 | torch.Size([120, 60]) || layers.0.residual_group.blocks.4.mlp.fc1.weight\n",
      " |  0.000 | -0.004 |  0.006 |  0.002 | torch.Size([120]) || layers.0.residual_group.blocks.4.mlp.fc1.bias\n",
      " | -0.000 | -0.078 |  0.075 |  0.020 | torch.Size([60, 120]) || layers.0.residual_group.blocks.4.mlp.fc2.weight\n",
      " | -0.000 | -0.004 |  0.004 |  0.002 | torch.Size([60]) || layers.0.residual_group.blocks.4.mlp.fc2.bias\n",
      " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.0.residual_group.blocks.5.attn_mask\n",
      " |  0.999 |  0.994 |  1.003 |  0.002 | torch.Size([60]) || layers.0.residual_group.blocks.5.norm1.weight\n",
      " |  0.000 | -0.004 |  0.004 |  0.002 | torch.Size([60]) || layers.0.residual_group.blocks.5.norm1.bias\n",
      " | -0.002 | -0.069 |  0.078 |  0.022 | torch.Size([225, 6]) || layers.0.residual_group.blocks.5.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.5.attn.relative_position_index\n",
      " |  0.000 | -0.075 |  0.072 |  0.020 | torch.Size([180, 60]) || layers.0.residual_group.blocks.5.attn.qkv.weight\n",
      " | -0.000 | -0.008 |  0.008 |  0.003 | torch.Size([180]) || layers.0.residual_group.blocks.5.attn.qkv.bias\n",
      " |  0.000 | -0.075 |  0.071 |  0.020 | torch.Size([60, 60]) || layers.0.residual_group.blocks.5.attn.proj.weight\n",
      " |  0.000 | -0.004 |  0.004 |  0.002 | torch.Size([60]) || layers.0.residual_group.blocks.5.attn.proj.bias\n",
      " |  1.000 |  0.993 |  1.009 |  0.003 | torch.Size([60]) || layers.0.residual_group.blocks.5.norm2.weight\n",
      " |  0.000 | -0.005 |  0.005 |  0.002 | torch.Size([60]) || layers.0.residual_group.blocks.5.norm2.bias\n",
      " |  0.000 | -0.067 |  0.073 |  0.020 | torch.Size([120, 60]) || layers.0.residual_group.blocks.5.mlp.fc1.weight\n",
      " |  0.000 | -0.004 |  0.003 |  0.002 | torch.Size([120]) || layers.0.residual_group.blocks.5.mlp.fc1.bias\n",
      " | -0.000 | -0.090 |  0.074 |  0.020 | torch.Size([60, 120]) || layers.0.residual_group.blocks.5.mlp.fc2.weight\n",
      " |  0.000 | -0.003 |  0.004 |  0.002 | torch.Size([60]) || layers.0.residual_group.blocks.5.mlp.fc2.bias\n",
      " | -0.000 | -0.053 |  0.053 |  0.025 | torch.Size([60, 60, 3, 3]) || layers.0.conv.weight\n",
      " | -0.003 | -0.042 |  0.036 |  0.022 | torch.Size([60]) || layers.0.conv.bias\n",
      " |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([60]) || layers.1.residual_group.blocks.0.norm1.weight\n",
      " |  0.000 | -0.004 |  0.005 |  0.002 | torch.Size([60]) || layers.1.residual_group.blocks.0.norm1.bias\n",
      " | -0.001 | -0.070 |  0.070 |  0.021 | torch.Size([225, 6]) || layers.1.residual_group.blocks.0.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.0.attn.relative_position_index\n",
      " | -0.000 | -0.083 |  0.082 |  0.020 | torch.Size([180, 60]) || layers.1.residual_group.blocks.0.attn.qkv.weight\n",
      " | -0.000 | -0.005 |  0.007 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.0.attn.qkv.bias\n",
      " | -0.000 | -0.073 |  0.060 |  0.020 | torch.Size([60, 60]) || layers.1.residual_group.blocks.0.attn.proj.weight\n",
      " |  0.000 | -0.003 |  0.004 |  0.002 | torch.Size([60]) || layers.1.residual_group.blocks.0.attn.proj.bias\n",
      " |  1.000 |  0.994 |  1.011 |  0.002 | torch.Size([60]) || layers.1.residual_group.blocks.0.norm2.weight\n",
      " | -0.000 | -0.005 |  0.003 |  0.002 | torch.Size([60]) || layers.1.residual_group.blocks.0.norm2.bias\n",
      " | -0.000 | -0.073 |  0.069 |  0.020 | torch.Size([120, 60]) || layers.1.residual_group.blocks.0.mlp.fc1.weight\n",
      " |  0.000 | -0.003 |  0.005 |  0.001 | torch.Size([120]) || layers.1.residual_group.blocks.0.mlp.fc1.bias\n",
      " |  0.000 | -0.076 |  0.080 |  0.020 | torch.Size([60, 120]) || layers.1.residual_group.blocks.0.mlp.fc2.weight\n",
      " |  0.000 | -0.003 |  0.004 |  0.001 | torch.Size([60]) || layers.1.residual_group.blocks.0.mlp.fc2.bias\n",
      " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.1.residual_group.blocks.1.attn_mask\n",
      " |  0.999 |  0.996 |  1.002 |  0.001 | torch.Size([60]) || layers.1.residual_group.blocks.1.norm1.weight\n",
      " | -0.000 | -0.004 |  0.004 |  0.002 | torch.Size([60]) || layers.1.residual_group.blocks.1.norm1.bias\n",
      " | -0.000 | -0.083 |  0.075 |  0.021 | torch.Size([225, 6]) || layers.1.residual_group.blocks.1.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.1.attn.relative_position_index\n",
      " | -0.000 | -0.085 |  0.073 |  0.020 | torch.Size([180, 60]) || layers.1.residual_group.blocks.1.attn.qkv.weight\n",
      " |  0.000 | -0.009 |  0.008 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.1.attn.qkv.bias\n",
      " |  0.001 | -0.067 |  0.078 |  0.020 | torch.Size([60, 60]) || layers.1.residual_group.blocks.1.attn.proj.weight\n",
      " |  0.000 | -0.004 |  0.004 |  0.002 | torch.Size([60]) || layers.1.residual_group.blocks.1.attn.proj.bias\n",
      " |  1.000 |  0.995 |  1.017 |  0.003 | torch.Size([60]) || layers.1.residual_group.blocks.1.norm2.weight\n",
      " | -0.000 | -0.004 |  0.005 |  0.002 | torch.Size([60]) || layers.1.residual_group.blocks.1.norm2.bias\n",
      " |  0.000 | -0.068 |  0.094 |  0.020 | torch.Size([120, 60]) || layers.1.residual_group.blocks.1.mlp.fc1.weight\n",
      " |  0.000 | -0.004 |  0.006 |  0.001 | torch.Size([120]) || layers.1.residual_group.blocks.1.mlp.fc1.bias\n",
      " |  0.000 | -0.085 |  0.075 |  0.020 | torch.Size([60, 120]) || layers.1.residual_group.blocks.1.mlp.fc2.weight\n",
      " | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([60]) || layers.1.residual_group.blocks.1.mlp.fc2.bias\n",
      " |  0.999 |  0.995 |  1.002 |  0.001 | torch.Size([60]) || layers.1.residual_group.blocks.2.norm1.weight\n",
      " | -0.000 | -0.005 |  0.003 |  0.002 | torch.Size([60]) || layers.1.residual_group.blocks.2.norm1.bias\n",
      " | -0.001 | -0.061 |  0.071 |  0.021 | torch.Size([225, 6]) || layers.1.residual_group.blocks.2.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.2.attn.relative_position_index\n",
      " |  0.000 | -0.082 |  0.076 |  0.020 | torch.Size([180, 60]) || layers.1.residual_group.blocks.2.attn.qkv.weight\n",
      " |  0.000 | -0.008 |  0.007 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.2.attn.qkv.bias\n",
      " | -0.000 | -0.081 |  0.078 |  0.020 | torch.Size([60, 60]) || layers.1.residual_group.blocks.2.attn.proj.weight\n",
      " |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([60]) || layers.1.residual_group.blocks.2.attn.proj.bias\n",
      " |  1.000 |  0.996 |  1.005 |  0.002 | torch.Size([60]) || layers.1.residual_group.blocks.2.norm2.weight\n",
      " |  0.000 | -0.004 |  0.004 |  0.002 | torch.Size([60]) || layers.1.residual_group.blocks.2.norm2.bias\n",
      " | -0.000 | -0.076 |  0.072 |  0.020 | torch.Size([120, 60]) || layers.1.residual_group.blocks.2.mlp.fc1.weight\n",
      " |  0.000 | -0.004 |  0.004 |  0.002 | torch.Size([120]) || layers.1.residual_group.blocks.2.mlp.fc1.bias\n",
      " | -0.000 | -0.070 |  0.086 |  0.020 | torch.Size([60, 120]) || layers.1.residual_group.blocks.2.mlp.fc2.weight\n",
      " |  0.000 | -0.003 |  0.004 |  0.002 | torch.Size([60]) || layers.1.residual_group.blocks.2.mlp.fc2.bias\n",
      " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.1.residual_group.blocks.3.attn_mask\n",
      " |  0.999 |  0.995 |  1.002 |  0.001 | torch.Size([60]) || layers.1.residual_group.blocks.3.norm1.weight\n",
      " |  0.000 | -0.004 |  0.004 |  0.002 | torch.Size([60]) || layers.1.residual_group.blocks.3.norm1.bias\n",
      " | -0.000 | -0.066 |  0.066 |  0.021 | torch.Size([225, 6]) || layers.1.residual_group.blocks.3.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.3.attn.relative_position_index\n",
      " | -0.000 | -0.078 |  0.073 |  0.020 | torch.Size([180, 60]) || layers.1.residual_group.blocks.3.attn.qkv.weight\n",
      " | -0.000 | -0.007 |  0.006 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.3.attn.qkv.bias\n",
      " | -0.000 | -0.087 |  0.070 |  0.020 | torch.Size([60, 60]) || layers.1.residual_group.blocks.3.attn.proj.weight\n",
      " |  0.000 | -0.003 |  0.004 |  0.001 | torch.Size([60]) || layers.1.residual_group.blocks.3.attn.proj.bias\n",
      " |  0.999 |  0.996 |  1.009 |  0.002 | torch.Size([60]) || layers.1.residual_group.blocks.3.norm2.weight\n",
      " |  0.000 | -0.005 |  0.004 |  0.002 | torch.Size([60]) || layers.1.residual_group.blocks.3.norm2.bias\n",
      " |  0.000 | -0.082 |  0.075 |  0.020 | torch.Size([120, 60]) || layers.1.residual_group.blocks.3.mlp.fc1.weight\n",
      " | -0.000 | -0.004 |  0.004 |  0.001 | torch.Size([120]) || layers.1.residual_group.blocks.3.mlp.fc1.bias\n",
      " |  0.000 | -0.087 |  0.074 |  0.020 | torch.Size([60, 120]) || layers.1.residual_group.blocks.3.mlp.fc2.weight\n",
      " |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([60]) || layers.1.residual_group.blocks.3.mlp.fc2.bias\n",
      " |  0.999 |  0.996 |  1.003 |  0.001 | torch.Size([60]) || layers.1.residual_group.blocks.4.norm1.weight\n",
      " | -0.000 | -0.003 |  0.004 |  0.001 | torch.Size([60]) || layers.1.residual_group.blocks.4.norm1.bias\n",
      " |  0.000 | -0.067 |  0.067 |  0.021 | torch.Size([225, 6]) || layers.1.residual_group.blocks.4.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.4.attn.relative_position_index\n",
      " | -0.000 | -0.078 |  0.082 |  0.020 | torch.Size([180, 60]) || layers.1.residual_group.blocks.4.attn.qkv.weight\n",
      " | -0.000 | -0.008 |  0.005 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.4.attn.qkv.bias\n",
      " | -0.001 | -0.066 |  0.076 |  0.020 | torch.Size([60, 60]) || layers.1.residual_group.blocks.4.attn.proj.weight\n",
      " |  0.000 | -0.004 |  0.003 |  0.001 | torch.Size([60]) || layers.1.residual_group.blocks.4.attn.proj.bias\n",
      " |  1.000 |  0.992 |  1.021 |  0.004 | torch.Size([60]) || layers.1.residual_group.blocks.4.norm2.weight\n",
      " | -0.000 | -0.004 |  0.004 |  0.002 | torch.Size([60]) || layers.1.residual_group.blocks.4.norm2.bias\n",
      " |  0.000 | -0.081 |  0.068 |  0.020 | torch.Size([120, 60]) || layers.1.residual_group.blocks.4.mlp.fc1.weight\n",
      " |  0.000 | -0.004 |  0.007 |  0.002 | torch.Size([120]) || layers.1.residual_group.blocks.4.mlp.fc1.bias\n",
      " | -0.001 | -0.079 |  0.069 |  0.020 | torch.Size([60, 120]) || layers.1.residual_group.blocks.4.mlp.fc2.weight\n",
      " | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([60]) || layers.1.residual_group.blocks.4.mlp.fc2.bias\n",
      " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.1.residual_group.blocks.5.attn_mask\n",
      " |  0.999 |  0.995 |  1.003 |  0.002 | torch.Size([60]) || layers.1.residual_group.blocks.5.norm1.weight\n",
      " |  0.000 | -0.004 |  0.004 |  0.002 | torch.Size([60]) || layers.1.residual_group.blocks.5.norm1.bias\n",
      " | -0.001 | -0.068 |  0.063 |  0.021 | torch.Size([225, 6]) || layers.1.residual_group.blocks.5.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.5.attn.relative_position_index\n",
      " |  0.000 | -0.070 |  0.080 |  0.020 | torch.Size([180, 60]) || layers.1.residual_group.blocks.5.attn.qkv.weight\n",
      " | -0.000 | -0.007 |  0.009 |  0.002 | torch.Size([180]) || layers.1.residual_group.blocks.5.attn.qkv.bias\n",
      " | -0.001 | -0.071 |  0.070 |  0.020 | torch.Size([60, 60]) || layers.1.residual_group.blocks.5.attn.proj.weight\n",
      " |  0.000 | -0.004 |  0.003 |  0.001 | torch.Size([60]) || layers.1.residual_group.blocks.5.attn.proj.bias\n",
      " |  1.000 |  0.995 |  1.007 |  0.002 | torch.Size([60]) || layers.1.residual_group.blocks.5.norm2.weight\n",
      " | -0.000 | -0.004 |  0.003 |  0.002 | torch.Size([60]) || layers.1.residual_group.blocks.5.norm2.bias\n",
      " |  0.000 | -0.076 |  0.075 |  0.020 | torch.Size([120, 60]) || layers.1.residual_group.blocks.5.mlp.fc1.weight\n",
      " |  0.000 | -0.003 |  0.004 |  0.001 | torch.Size([120]) || layers.1.residual_group.blocks.5.mlp.fc1.bias\n",
      " | -0.000 | -0.069 |  0.074 |  0.020 | torch.Size([60, 120]) || layers.1.residual_group.blocks.5.mlp.fc2.weight\n",
      " |  0.000 | -0.004 |  0.004 |  0.002 | torch.Size([60]) || layers.1.residual_group.blocks.5.mlp.fc2.bias\n",
      " |  0.000 | -0.061 |  0.065 |  0.025 | torch.Size([60, 60, 3, 3]) || layers.1.conv.weight\n",
      " | -0.000 | -0.042 |  0.042 |  0.024 | torch.Size([60]) || layers.1.conv.bias\n",
      " |  0.999 |  0.996 |  1.003 |  0.002 | torch.Size([60]) || layers.2.residual_group.blocks.0.norm1.weight\n",
      " |  0.000 | -0.003 |  0.004 |  0.001 | torch.Size([60]) || layers.2.residual_group.blocks.0.norm1.bias\n",
      " |  0.000 | -0.070 |  0.063 |  0.021 | torch.Size([225, 6]) || layers.2.residual_group.blocks.0.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.0.attn.relative_position_index\n",
      " | -0.000 | -0.080 |  0.081 |  0.020 | torch.Size([180, 60]) || layers.2.residual_group.blocks.0.attn.qkv.weight\n",
      " |  0.000 | -0.004 |  0.006 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.0.attn.qkv.bias\n",
      " |  0.000 | -0.071 |  0.074 |  0.020 | torch.Size([60, 60]) || layers.2.residual_group.blocks.0.attn.proj.weight\n",
      " | -0.000 | -0.005 |  0.004 |  0.002 | torch.Size([60]) || layers.2.residual_group.blocks.0.attn.proj.bias\n",
      " |  1.000 |  0.995 |  1.009 |  0.002 | torch.Size([60]) || layers.2.residual_group.blocks.0.norm2.weight\n",
      " |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([60]) || layers.2.residual_group.blocks.0.norm2.bias\n",
      " | -0.000 | -0.076 |  0.067 |  0.020 | torch.Size([120, 60]) || layers.2.residual_group.blocks.0.mlp.fc1.weight\n",
      " | -0.000 | -0.004 |  0.004 |  0.001 | torch.Size([120]) || layers.2.residual_group.blocks.0.mlp.fc1.bias\n",
      " | -0.000 | -0.078 |  0.081 |  0.020 | torch.Size([60, 120]) || layers.2.residual_group.blocks.0.mlp.fc2.weight\n",
      " | -0.000 | -0.003 |  0.002 |  0.001 | torch.Size([60]) || layers.2.residual_group.blocks.0.mlp.fc2.bias\n",
      " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.2.residual_group.blocks.1.attn_mask\n",
      " |  1.000 |  0.997 |  1.003 |  0.001 | torch.Size([60]) || layers.2.residual_group.blocks.1.norm1.weight\n",
      " |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([60]) || layers.2.residual_group.blocks.1.norm1.bias\n",
      " | -0.001 | -0.077 |  0.067 |  0.021 | torch.Size([225, 6]) || layers.2.residual_group.blocks.1.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.1.attn.relative_position_index\n",
      " | -0.000 | -0.070 |  0.071 |  0.020 | torch.Size([180, 60]) || layers.2.residual_group.blocks.1.attn.qkv.weight\n",
      " |  0.000 | -0.011 |  0.007 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.1.attn.qkv.bias\n",
      " |  0.000 | -0.067 |  0.066 |  0.020 | torch.Size([60, 60]) || layers.2.residual_group.blocks.1.attn.proj.weight\n",
      " |  0.000 | -0.003 |  0.002 |  0.001 | torch.Size([60]) || layers.2.residual_group.blocks.1.attn.proj.bias\n",
      " |  1.000 |  0.996 |  1.012 |  0.003 | torch.Size([60]) || layers.2.residual_group.blocks.1.norm2.weight\n",
      " | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([60]) || layers.2.residual_group.blocks.1.norm2.bias\n",
      " | -0.000 | -0.075 |  0.073 |  0.020 | torch.Size([120, 60]) || layers.2.residual_group.blocks.1.mlp.fc1.weight\n",
      " | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([120]) || layers.2.residual_group.blocks.1.mlp.fc1.bias\n",
      " | -0.000 | -0.079 |  0.068 |  0.020 | torch.Size([60, 120]) || layers.2.residual_group.blocks.1.mlp.fc2.weight\n",
      " | -0.000 | -0.003 |  0.002 |  0.001 | torch.Size([60]) || layers.2.residual_group.blocks.1.mlp.fc2.bias\n",
      " |  0.999 |  0.997 |  1.002 |  0.001 | torch.Size([60]) || layers.2.residual_group.blocks.2.norm1.weight\n",
      " |  0.000 | -0.003 |  0.004 |  0.001 | torch.Size([60]) || layers.2.residual_group.blocks.2.norm1.bias\n",
      " | -0.000 | -0.066 |  0.061 |  0.021 | torch.Size([225, 6]) || layers.2.residual_group.blocks.2.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.2.attn.relative_position_index\n",
      " | -0.000 | -0.093 |  0.083 |  0.020 | torch.Size([180, 60]) || layers.2.residual_group.blocks.2.attn.qkv.weight\n",
      " | -0.000 | -0.004 |  0.003 |  0.001 | torch.Size([180]) || layers.2.residual_group.blocks.2.attn.qkv.bias\n",
      " |  0.000 | -0.070 |  0.062 |  0.020 | torch.Size([60, 60]) || layers.2.residual_group.blocks.2.attn.proj.weight\n",
      " |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([60]) || layers.2.residual_group.blocks.2.attn.proj.bias\n",
      " |  0.999 |  0.995 |  1.004 |  0.002 | torch.Size([60]) || layers.2.residual_group.blocks.2.norm2.weight\n",
      " |  0.000 | -0.004 |  0.004 |  0.002 | torch.Size([60]) || layers.2.residual_group.blocks.2.norm2.bias\n",
      " |  0.000 | -0.069 |  0.068 |  0.020 | torch.Size([120, 60]) || layers.2.residual_group.blocks.2.mlp.fc1.weight\n",
      " | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([120]) || layers.2.residual_group.blocks.2.mlp.fc1.bias\n",
      " | -0.000 | -0.068 |  0.076 |  0.020 | torch.Size([60, 120]) || layers.2.residual_group.blocks.2.mlp.fc2.weight\n",
      " | -0.000 | -0.003 |  0.002 |  0.001 | torch.Size([60]) || layers.2.residual_group.blocks.2.mlp.fc2.bias\n",
      " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.2.residual_group.blocks.3.attn_mask\n",
      " |  0.999 |  0.996 |  1.003 |  0.001 | torch.Size([60]) || layers.2.residual_group.blocks.3.norm1.weight\n",
      " |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([60]) || layers.2.residual_group.blocks.3.norm1.bias\n",
      " | -0.001 | -0.058 |  0.077 |  0.021 | torch.Size([225, 6]) || layers.2.residual_group.blocks.3.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.3.attn.relative_position_index\n",
      " |  0.000 | -0.094 |  0.090 |  0.020 | torch.Size([180, 60]) || layers.2.residual_group.blocks.3.attn.qkv.weight\n",
      " | -0.000 | -0.011 |  0.006 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.3.attn.qkv.bias\n",
      " |  0.000 | -0.065 |  0.074 |  0.020 | torch.Size([60, 60]) || layers.2.residual_group.blocks.3.attn.proj.weight\n",
      " | -0.000 | -0.003 |  0.002 |  0.001 | torch.Size([60]) || layers.2.residual_group.blocks.3.attn.proj.bias\n",
      " |  1.000 |  0.995 |  1.005 |  0.002 | torch.Size([60]) || layers.2.residual_group.blocks.3.norm2.weight\n",
      " |  0.000 | -0.003 |  0.004 |  0.001 | torch.Size([60]) || layers.2.residual_group.blocks.3.norm2.bias\n",
      " | -0.000 | -0.071 |  0.077 |  0.020 | torch.Size([120, 60]) || layers.2.residual_group.blocks.3.mlp.fc1.weight\n",
      " | -0.000 | -0.003 |  0.005 |  0.001 | torch.Size([120]) || layers.2.residual_group.blocks.3.mlp.fc1.bias\n",
      " | -0.000 | -0.065 |  0.073 |  0.020 | torch.Size([60, 120]) || layers.2.residual_group.blocks.3.mlp.fc2.weight\n",
      " | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([60]) || layers.2.residual_group.blocks.3.mlp.fc2.bias\n",
      " |  0.999 |  0.995 |  1.003 |  0.002 | torch.Size([60]) || layers.2.residual_group.blocks.4.norm1.weight\n",
      " |  0.000 | -0.005 |  0.005 |  0.002 | torch.Size([60]) || layers.2.residual_group.blocks.4.norm1.bias\n",
      " |  0.001 | -0.082 |  0.063 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.4.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.4.attn.relative_position_index\n",
      " |  0.000 | -0.082 |  0.075 |  0.020 | torch.Size([180, 60]) || layers.2.residual_group.blocks.4.attn.qkv.weight\n",
      " | -0.000 | -0.006 |  0.006 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.4.attn.qkv.bias\n",
      " | -0.000 | -0.071 |  0.073 |  0.020 | torch.Size([60, 60]) || layers.2.residual_group.blocks.4.attn.proj.weight\n",
      " | -0.000 | -0.003 |  0.002 |  0.001 | torch.Size([60]) || layers.2.residual_group.blocks.4.attn.proj.bias\n",
      " |  1.000 |  0.996 |  1.004 |  0.002 | torch.Size([60]) || layers.2.residual_group.blocks.4.norm2.weight\n",
      " |  0.000 | -0.005 |  0.004 |  0.002 | torch.Size([60]) || layers.2.residual_group.blocks.4.norm2.bias\n",
      " |  0.000 | -0.071 |  0.071 |  0.020 | torch.Size([120, 60]) || layers.2.residual_group.blocks.4.mlp.fc1.weight\n",
      " |  0.000 | -0.002 |  0.003 |  0.001 | torch.Size([120]) || layers.2.residual_group.blocks.4.mlp.fc1.bias\n",
      " |  0.000 | -0.072 |  0.083 |  0.020 | torch.Size([60, 120]) || layers.2.residual_group.blocks.4.mlp.fc2.weight\n",
      " | -0.000 | -0.003 |  0.002 |  0.001 | torch.Size([60]) || layers.2.residual_group.blocks.4.mlp.fc2.bias\n",
      " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.2.residual_group.blocks.5.attn_mask\n",
      " |  1.000 |  0.997 |  1.004 |  0.001 | torch.Size([60]) || layers.2.residual_group.blocks.5.norm1.weight\n",
      " | -0.000 | -0.002 |  0.002 |  0.001 | torch.Size([60]) || layers.2.residual_group.blocks.5.norm1.bias\n",
      " | -0.001 | -0.066 |  0.067 |  0.021 | torch.Size([225, 6]) || layers.2.residual_group.blocks.5.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.5.attn.relative_position_index\n",
      " |  0.000 | -0.074 |  0.075 |  0.020 | torch.Size([180, 60]) || layers.2.residual_group.blocks.5.attn.qkv.weight\n",
      " | -0.000 | -0.007 |  0.012 |  0.002 | torch.Size([180]) || layers.2.residual_group.blocks.5.attn.qkv.bias\n",
      " | -0.000 | -0.067 |  0.063 |  0.020 | torch.Size([60, 60]) || layers.2.residual_group.blocks.5.attn.proj.weight\n",
      " | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([60]) || layers.2.residual_group.blocks.5.attn.proj.bias\n",
      " |  0.999 |  0.995 |  1.003 |  0.002 | torch.Size([60]) || layers.2.residual_group.blocks.5.norm2.weight\n",
      " |  0.000 | -0.003 |  0.005 |  0.002 | torch.Size([60]) || layers.2.residual_group.blocks.5.norm2.bias\n",
      " |  0.000 | -0.076 |  0.087 |  0.020 | torch.Size([120, 60]) || layers.2.residual_group.blocks.5.mlp.fc1.weight\n",
      " |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([120]) || layers.2.residual_group.blocks.5.mlp.fc1.bias\n",
      " |  0.000 | -0.077 |  0.069 |  0.020 | torch.Size([60, 120]) || layers.2.residual_group.blocks.5.mlp.fc2.weight\n",
      " | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([60]) || layers.2.residual_group.blocks.5.mlp.fc2.bias\n",
      " | -0.000 | -0.053 |  0.055 |  0.025 | torch.Size([60, 60, 3, 3]) || layers.2.conv.weight\n",
      " | -0.002 | -0.044 |  0.044 |  0.027 | torch.Size([60]) || layers.2.conv.bias\n",
      " |  0.999 |  0.995 |  1.002 |  0.002 | torch.Size([60]) || layers.3.residual_group.blocks.0.norm1.weight\n",
      " |  0.000 | -0.004 |  0.005 |  0.002 | torch.Size([60]) || layers.3.residual_group.blocks.0.norm1.bias\n",
      " | -0.000 | -0.081 |  0.069 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.0.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.0.attn.relative_position_index\n",
      " | -0.000 | -0.076 |  0.073 |  0.020 | torch.Size([180, 60]) || layers.3.residual_group.blocks.0.attn.qkv.weight\n",
      " | -0.000 | -0.006 |  0.004 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.0.attn.qkv.bias\n",
      " | -0.000 | -0.077 |  0.082 |  0.020 | torch.Size([60, 60]) || layers.3.residual_group.blocks.0.attn.proj.weight\n",
      " | -0.000 | -0.005 |  0.002 |  0.001 | torch.Size([60]) || layers.3.residual_group.blocks.0.attn.proj.bias\n",
      " |  0.999 |  0.996 |  1.004 |  0.002 | torch.Size([60]) || layers.3.residual_group.blocks.0.norm2.weight\n",
      " |  0.000 | -0.004 |  0.003 |  0.001 | torch.Size([60]) || layers.3.residual_group.blocks.0.norm2.bias\n",
      " |  0.000 | -0.072 |  0.071 |  0.020 | torch.Size([120, 60]) || layers.3.residual_group.blocks.0.mlp.fc1.weight\n",
      " | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([120]) || layers.3.residual_group.blocks.0.mlp.fc1.bias\n",
      " | -0.000 | -0.092 |  0.070 |  0.020 | torch.Size([60, 120]) || layers.3.residual_group.blocks.0.mlp.fc2.weight\n",
      " | -0.000 | -0.005 |  0.002 |  0.001 | torch.Size([60]) || layers.3.residual_group.blocks.0.mlp.fc2.bias\n",
      " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.3.residual_group.blocks.1.attn_mask\n",
      " |  1.000 |  0.997 |  1.004 |  0.001 | torch.Size([60]) || layers.3.residual_group.blocks.1.norm1.weight\n",
      " | -0.000 | -0.003 |  0.002 |  0.001 | torch.Size([60]) || layers.3.residual_group.blocks.1.norm1.bias\n",
      " |  0.000 | -0.053 |  0.073 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.1.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.1.attn.relative_position_index\n",
      " | -0.000 | -0.076 |  0.077 |  0.020 | torch.Size([180, 60]) || layers.3.residual_group.blocks.1.attn.qkv.weight\n",
      " |  0.000 | -0.005 |  0.005 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.1.attn.qkv.bias\n",
      " | -0.000 | -0.069 |  0.068 |  0.020 | torch.Size([60, 60]) || layers.3.residual_group.blocks.1.attn.proj.weight\n",
      " | -0.000 | -0.004 |  0.003 |  0.001 | torch.Size([60]) || layers.3.residual_group.blocks.1.attn.proj.bias\n",
      " |  1.000 |  0.997 |  1.006 |  0.002 | torch.Size([60]) || layers.3.residual_group.blocks.1.norm2.weight\n",
      " |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([60]) || layers.3.residual_group.blocks.1.norm2.bias\n",
      " |  0.000 | -0.064 |  0.073 |  0.020 | torch.Size([120, 60]) || layers.3.residual_group.blocks.1.mlp.fc1.weight\n",
      " | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([120]) || layers.3.residual_group.blocks.1.mlp.fc1.bias\n",
      " | -0.000 | -0.073 |  0.073 |  0.020 | torch.Size([60, 120]) || layers.3.residual_group.blocks.1.mlp.fc2.weight\n",
      " | -0.000 | -0.004 |  0.002 |  0.001 | torch.Size([60]) || layers.3.residual_group.blocks.1.mlp.fc2.bias\n",
      " |  0.999 |  0.995 |  1.004 |  0.002 | torch.Size([60]) || layers.3.residual_group.blocks.2.norm1.weight\n",
      " |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([60]) || layers.3.residual_group.blocks.2.norm1.bias\n",
      " |  0.001 | -0.064 |  0.077 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.2.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.2.attn.relative_position_index\n",
      " |  0.000 | -0.073 |  0.079 |  0.020 | torch.Size([180, 60]) || layers.3.residual_group.blocks.2.attn.qkv.weight\n",
      " | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.2.attn.qkv.bias\n",
      " |  0.000 | -0.074 |  0.077 |  0.020 | torch.Size([60, 60]) || layers.3.residual_group.blocks.2.attn.proj.weight\n",
      " | -0.000 | -0.004 |  0.003 |  0.001 | torch.Size([60]) || layers.3.residual_group.blocks.2.attn.proj.bias\n",
      " |  1.000 |  0.995 |  1.004 |  0.002 | torch.Size([60]) || layers.3.residual_group.blocks.2.norm2.weight\n",
      " |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([60]) || layers.3.residual_group.blocks.2.norm2.bias\n",
      " |  0.000 | -0.077 |  0.071 |  0.020 | torch.Size([120, 60]) || layers.3.residual_group.blocks.2.mlp.fc1.weight\n",
      " | -0.000 | -0.002 |  0.002 |  0.001 | torch.Size([120]) || layers.3.residual_group.blocks.2.mlp.fc1.bias\n",
      " |  0.000 | -0.075 |  0.077 |  0.020 | torch.Size([60, 120]) || layers.3.residual_group.blocks.2.mlp.fc2.weight\n",
      " | -0.000 | -0.004 |  0.002 |  0.001 | torch.Size([60]) || layers.3.residual_group.blocks.2.mlp.fc2.bias\n",
      " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.3.residual_group.blocks.3.attn_mask\n",
      " |  0.999 |  0.997 |  1.003 |  0.001 | torch.Size([60]) || layers.3.residual_group.blocks.3.norm1.weight\n",
      " |  0.000 | -0.003 |  0.003 |  0.002 | torch.Size([60]) || layers.3.residual_group.blocks.3.norm1.bias\n",
      " | -0.001 | -0.065 |  0.075 |  0.021 | torch.Size([225, 6]) || layers.3.residual_group.blocks.3.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.3.attn.relative_position_index\n",
      " |  0.000 | -0.078 |  0.074 |  0.020 | torch.Size([180, 60]) || layers.3.residual_group.blocks.3.attn.qkv.weight\n",
      " | -0.000 | -0.005 |  0.004 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.3.attn.qkv.bias\n",
      " |  0.001 | -0.074 |  0.075 |  0.020 | torch.Size([60, 60]) || layers.3.residual_group.blocks.3.attn.proj.weight\n",
      " | -0.000 | -0.003 |  0.003 |  0.001 | torch.Size([60]) || layers.3.residual_group.blocks.3.attn.proj.bias\n",
      " |  1.000 |  0.995 |  1.007 |  0.002 | torch.Size([60]) || layers.3.residual_group.blocks.3.norm2.weight\n",
      " | -0.000 | -0.004 |  0.002 |  0.001 | torch.Size([60]) || layers.3.residual_group.blocks.3.norm2.bias\n",
      " | -0.000 | -0.072 |  0.070 |  0.020 | torch.Size([120, 60]) || layers.3.residual_group.blocks.3.mlp.fc1.weight\n",
      " | -0.000 | -0.003 |  0.004 |  0.001 | torch.Size([120]) || layers.3.residual_group.blocks.3.mlp.fc1.bias\n",
      " | -0.000 | -0.088 |  0.072 |  0.020 | torch.Size([60, 120]) || layers.3.residual_group.blocks.3.mlp.fc2.weight\n",
      " | -0.000 | -0.002 |  0.002 |  0.001 | torch.Size([60]) || layers.3.residual_group.blocks.3.mlp.fc2.bias\n",
      " |  1.000 |  0.996 |  1.003 |  0.002 | torch.Size([60]) || layers.3.residual_group.blocks.4.norm1.weight\n",
      " | -0.000 | -0.003 |  0.002 |  0.002 | torch.Size([60]) || layers.3.residual_group.blocks.4.norm1.bias\n",
      " |  0.000 | -0.072 |  0.064 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.4.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.4.attn.relative_position_index\n",
      " |  0.000 | -0.076 |  0.075 |  0.020 | torch.Size([180, 60]) || layers.3.residual_group.blocks.4.attn.qkv.weight\n",
      " |  0.000 | -0.004 |  0.005 |  0.001 | torch.Size([180]) || layers.3.residual_group.blocks.4.attn.qkv.bias\n",
      " |  0.000 | -0.076 |  0.068 |  0.020 | torch.Size([60, 60]) || layers.3.residual_group.blocks.4.attn.proj.weight\n",
      " |  0.000 | -0.005 |  0.003 |  0.002 | torch.Size([60]) || layers.3.residual_group.blocks.4.attn.proj.bias\n",
      " |  1.000 |  0.997 |  1.004 |  0.002 | torch.Size([60]) || layers.3.residual_group.blocks.4.norm2.weight\n",
      " | -0.000 | -0.004 |  0.002 |  0.001 | torch.Size([60]) || layers.3.residual_group.blocks.4.norm2.bias\n",
      " |  0.000 | -0.079 |  0.070 |  0.020 | torch.Size([120, 60]) || layers.3.residual_group.blocks.4.mlp.fc1.weight\n",
      " |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([120]) || layers.3.residual_group.blocks.4.mlp.fc1.bias\n",
      " |  0.000 | -0.073 |  0.070 |  0.020 | torch.Size([60, 120]) || layers.3.residual_group.blocks.4.mlp.fc2.weight\n",
      " | -0.000 | -0.004 |  0.002 |  0.001 | torch.Size([60]) || layers.3.residual_group.blocks.4.mlp.fc2.bias\n",
      " | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.3.residual_group.blocks.5.attn_mask\n",
      " |  0.999 |  0.996 |  1.002 |  0.001 | torch.Size([60]) || layers.3.residual_group.blocks.5.norm1.weight\n",
      " |  0.000 | -0.003 |  0.003 |  0.001 | torch.Size([60]) || layers.3.residual_group.blocks.5.norm1.bias\n",
      " |  0.000 | -0.063 |  0.065 |  0.021 | torch.Size([225, 6]) || layers.3.residual_group.blocks.5.attn.relative_position_bias_table\n",
      " | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.5.attn.relative_position_index\n",
      " | -0.000 | -0.071 |  0.075 |  0.020 | torch.Size([180, 60]) || layers.3.residual_group.blocks.5.attn.qkv.weight\n",
      " | -0.000 | -0.008 |  0.007 |  0.002 | torch.Size([180]) || layers.3.residual_group.blocks.5.attn.qkv.bias\n",
      " |  0.000 | -0.077 |  0.073 |  0.020 | torch.Size([60, 60]) || layers.3.residual_group.blocks.5.attn.proj.weight\n",
      " | -0.000 | -0.004 |  0.003 |  0.001 | torch.Size([60]) || layers.3.residual_group.blocks.5.attn.proj.bias\n",
      " |  0.999 |  0.994 |  1.003 |  0.002 | torch.Size([60]) || layers.3.residual_group.blocks.5.norm2.weight\n",
      " |  0.000 | -0.003 |  0.004 |  0.002 | torch.Size([60]) || layers.3.residual_group.blocks.5.norm2.bias\n",
      " |  0.000 | -0.074 |  0.074 |  0.020 | torch.Size([120, 60]) || layers.3.residual_group.blocks.5.mlp.fc1.weight\n",
      " |  0.000 | -0.002 |  0.003 |  0.001 | torch.Size([120]) || layers.3.residual_group.blocks.5.mlp.fc1.bias\n",
      " |  0.000 | -0.084 |  0.075 |  0.020 | torch.Size([60, 120]) || layers.3.residual_group.blocks.5.mlp.fc2.weight\n",
      " | -0.000 | -0.004 |  0.003 |  0.001 | torch.Size([60]) || layers.3.residual_group.blocks.5.mlp.fc2.bias\n",
      " | -0.000 | -0.051 |  0.058 |  0.025 | torch.Size([60, 60, 3, 3]) || layers.3.conv.weight\n",
      " | -0.000 | -0.041 |  0.043 |  0.025 | torch.Size([60]) || layers.3.conv.bias\n",
      " |  0.997 |  0.993 |  1.004 |  0.002 | torch.Size([60]) || norm.weight\n",
      " | -0.000 | -0.004 |  0.005 |  0.002 | torch.Size([60]) || norm.bias\n",
      " |  0.000 | -0.049 |  0.054 |  0.025 | torch.Size([60, 60, 3, 3]) || conv_after_body.weight\n",
      " |  0.002 | -0.040 |  0.044 |  0.023 | torch.Size([60]) || conv_after_body.bias\n",
      " |  0.000 | -0.052 |  0.051 |  0.025 | torch.Size([12, 60, 3, 3]) || upsample.0.weight\n",
      " |  0.003 | -0.038 |  0.039 |  0.027 | torch.Size([12]) || upsample.0.bias\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "# ----------------------------------------\n",
    "# Step--3 (initialize model)\n",
    "# ----------------------------------------\n",
    "'''\n",
    "\n",
    "model = define_Model(opt)\n",
    "model.init_train()\n",
    "if opt['rank'] == 0:\n",
    "    logger.info(model.info_network())\n",
    "    logger.info(model.info_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d041d769",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-12-11 16:01:18.877 : <epoch:  0, iter:     332, lr:2.000e-04> G_loss: 3.097e-02 \n",
      "22-12-11 16:01:18.878 : Saving the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superresolution\\swinir_sr_lightweight_x2\\images\\baby\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-12-11 16:01:25.165 : ---1-->   baby.png | 30.36dB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superresolution\\swinir_sr_lightweight_x2\\images\\bird\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-12-11 16:01:25.840 : ---2-->   bird.png | 29.05dB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superresolution\\swinir_sr_lightweight_x2\\images\\butterfly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-12-11 16:01:26.275 : ---3--> butterfly.png | 24.58dB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superresolution\\swinir_sr_lightweight_x2\\images\\head\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-12-11 16:01:26.811 : ---4-->   head.png | 29.76dB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superresolution\\swinir_sr_lightweight_x2\\images\\woman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-12-11 16:01:27.334 : ---5-->  woman.png | 28.94dB\n",
      "22-12-11 16:01:27.731 : <epoch:  0, iter:     332, Average PSNR : 28.54dB\n",
      "\n",
      "22-12-11 16:01:31.895 : <epoch:  1, iter:     334, lr:2.000e-04> G_loss: 2.596e-02 \n",
      "22-12-11 16:01:35.933 : <epoch:  2, iter:     336, lr:2.000e-04> G_loss: 2.988e-02 \n",
      "22-12-11 16:01:35.934 : Saving the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superresolution\\swinir_sr_lightweight_x2\\images\\baby\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-12-11 16:01:41.277 : ---1-->   baby.png | 32.95dB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superresolution\\swinir_sr_lightweight_x2\\images\\bird\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-12-11 16:01:41.920 : ---2-->   bird.png | 29.32dB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superresolution\\swinir_sr_lightweight_x2\\images\\butterfly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-12-11 16:01:42.432 : ---3--> butterfly.png | 25.03dB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superresolution\\swinir_sr_lightweight_x2\\images\\head\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-12-11 16:01:42.977 : ---4-->   head.png | 29.79dB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superresolution\\swinir_sr_lightweight_x2\\images\\woman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-12-11 16:01:43.499 : ---5-->  woman.png | 29.47dB\n",
      "22-12-11 16:01:43.801 : <epoch:  2, iter:     336, Average PSNR : 29.31dB\n",
      "\n",
      "22-12-11 16:01:47.831 : <epoch:  3, iter:     338, lr:2.000e-04> G_loss: 3.255e-02 \n",
      "22-12-11 16:01:51.977 : <epoch:  4, iter:     340, lr:2.000e-04> G_loss: 3.105e-02 \n",
      "22-12-11 16:01:51.979 : Saving the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superresolution\\swinir_sr_lightweight_x2\\images\\baby\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-12-11 16:01:57.327 : ---1-->   baby.png | 31.82dB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superresolution\\swinir_sr_lightweight_x2\\images\\bird\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-12-11 16:01:57.869 : ---2-->   bird.png | 28.96dB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superresolution\\swinir_sr_lightweight_x2\\images\\butterfly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-12-11 16:01:58.307 : ---3--> butterfly.png | 24.94dB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superresolution\\swinir_sr_lightweight_x2\\images\\head\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-12-11 16:01:59.117 : ---4-->   head.png | 29.59dB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superresolution\\swinir_sr_lightweight_x2\\images\\woman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-12-11 16:01:59.913 : ---5-->  woman.png | 29.04dB\n",
      "22-12-11 16:02:00.229 : <epoch:  4, iter:     340, Average PSNR : 28.87dB\n",
      "\n",
      "22-12-11 16:02:04.346 : <epoch:  5, iter:     342, lr:2.000e-04> G_loss: 2.939e-02 \n",
      "22-12-11 16:02:08.369 : <epoch:  6, iter:     344, lr:2.000e-04> G_loss: 2.120e-02 \n",
      "22-12-11 16:02:08.371 : Saving the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superresolution\\swinir_sr_lightweight_x2\\images\\baby\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-12-11 16:02:14.049 : ---1-->   baby.png | 31.32dB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superresolution\\swinir_sr_lightweight_x2\\images\\bird\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-12-11 16:02:14.603 : ---2-->   bird.png | 29.15dB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superresolution\\swinir_sr_lightweight_x2\\images\\butterfly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-12-11 16:02:15.031 : ---3--> butterfly.png | 25.13dB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superresolution\\swinir_sr_lightweight_x2\\images\\head\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-12-11 16:02:15.716 : ---4-->   head.png | 28.97dB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superresolution\\swinir_sr_lightweight_x2\\images\\woman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-12-11 16:02:16.470 : ---5-->  woman.png | 28.81dB\n",
      "22-12-11 16:02:16.768 : <epoch:  6, iter:     344, Average PSNR : 28.68dB\n",
      "\n",
      "22-12-11 16:02:20.882 : <epoch:  7, iter:     346, lr:2.000e-04> G_loss: 3.028e-02 \n",
      "22-12-11 16:02:24.581 : <epoch:  8, iter:     348, lr:2.000e-04> G_loss: 3.618e-02 \n",
      "22-12-11 16:02:24.582 : Saving the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superresolution\\swinir_sr_lightweight_x2\\images\\baby\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-12-11 16:02:29.923 : ---1-->   baby.png | 32.28dB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superresolution\\swinir_sr_lightweight_x2\\images\\bird\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-12-11 16:02:30.472 : ---2-->   bird.png | 28.99dB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superresolution\\swinir_sr_lightweight_x2\\images\\butterfly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-12-11 16:02:30.902 : ---3--> butterfly.png | 24.99dB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superresolution\\swinir_sr_lightweight_x2\\images\\head\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-12-11 16:02:31.544 : ---4-->   head.png | 29.32dB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superresolution\\swinir_sr_lightweight_x2\\images\\woman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-12-11 16:02:32.208 : ---5-->  woman.png | 29.26dB\n",
      "22-12-11 16:02:32.499 : <epoch:  8, iter:     348, Average PSNR : 28.97dB\n",
      "\n",
      "22-12-11 16:02:36.442 : <epoch:  9, iter:     350, lr:2.000e-04> G_loss: 3.525e-02 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "    '''\n",
    "    # ----------------------------------------\n",
    "    # Step--4 (main training)\n",
    "    # ----------------------------------------\n",
    "    '''\n",
    "\n",
    "    logmyloss = torch.Tensor()\n",
    "    logmypsnr = torch.Tensor()\n",
    "    for epoch in range(10):  # keep running\n",
    "        if opt['dist']:\n",
    "            train_sampler.set_epoch(epoch)\n",
    "\n",
    "        for i, train_data in enumerate(train_loader):\n",
    "\n",
    "            current_step += 1\n",
    "\n",
    "            # -------------------------------\n",
    "            # 1) update learning rate\n",
    "            # -------------------------------\n",
    "            model.update_learning_rate(current_step)\n",
    "\n",
    "            # -------------------------------\n",
    "            # 2) feed patch pairs\n",
    "            # -------------------------------\n",
    "            model.feed_data(train_data)\n",
    "\n",
    "            # -------------------------------\n",
    "            # 3) optimize parameters\n",
    "            # -------------------------------\n",
    "            model.optimize_parameters(current_step)\n",
    "\n",
    "            # -------------------------------\n",
    "            # 4) training information\n",
    "            # -------------------------------\n",
    "            if current_step % opt['train']['checkpoint_print'] == 0 and opt['rank'] == 0:\n",
    "                logs = model.current_log()  # such as loss\n",
    "                message = '<epoch:{:3d}, iter:{:8,d}, lr:{:.3e}> '.format(epoch, current_step, model.current_learning_rate())\n",
    "                for k, v in logs.items():  # merge log information into message\n",
    "                    message += '{:s}: {:.3e} '.format(k, v)\n",
    "                    global logmyloss\n",
    "                    logmyloss = torch.cat([logmyloss, torch.tensor([v])])\n",
    "                    torch.save(logmyloss, os.path.join(\"loss\", 'last_loss.pth'))\n",
    "                    \n",
    "                logger.info(message)\n",
    "\n",
    "            # -------------------------------\n",
    "            # 5) save model\n",
    "            # -------------------------------\n",
    "            if current_step % opt['train']['checkpoint_save'] == 0 and opt['rank'] == 0:\n",
    "                logger.info('Saving the model.')\n",
    "                model.save(current_step)\n",
    "\n",
    "            # -------------------------------\n",
    "            # 6) testing\n",
    "            # -------------------------------\n",
    "            if current_step % opt['train']['checkpoint_test'] == 0 and opt['rank'] == 0:\n",
    "\n",
    "                avg_psnr = 0.0\n",
    "                idx = 0\n",
    "\n",
    "                for test_data in test_loader:\n",
    "                    idx += 1\n",
    "                    image_name_ext = os.path.basename(test_data['L_path'][0])\n",
    "                    img_name, ext = os.path.splitext(image_name_ext)\n",
    "\n",
    "                    img_dir = os.path.join(opt['path']['images'], img_name)\n",
    "                    util.mkdir(img_dir)\n",
    "                    print(img_dir)\n",
    "\n",
    "                    model.feed_data(test_data)\n",
    "                    model.test()\n",
    "\n",
    "                    visuals = model.current_visuals()\n",
    "                    E_img = util.tensor2uint(visuals['E'])\n",
    "                    H_img = util.tensor2uint(visuals['H'])\n",
    "\n",
    "                    # -----------------------\n",
    "                    # save estimated image E\n",
    "                    # -----------------------\n",
    "                    save_img_path = os.path.join(img_dir, '{:s}_{:d}.png'.format(img_name, current_step))\n",
    "                    util.imsave(E_img, save_img_path)\n",
    "\n",
    "                    # -----------------------\n",
    "                    # calculate PSNR\n",
    "                    # -----------------------\n",
    "                    current_psnr = util.calculate_psnr(E_img, H_img, border=border)\n",
    "\n",
    "                    logger.info('{:->4d}--> {:>10s} | {:<4.2f}dB'.format(idx, image_name_ext, current_psnr))\n",
    "\n",
    "                    avg_psnr += current_psnr\n",
    "\n",
    "                avg_psnr = avg_psnr / idx\n",
    "\n",
    "                # testing log\n",
    "                logger.info('<epoch:{:3d}, iter:{:8,d}, Average PSNR : {:<.2f}dB\\n'.format(epoch, current_step, avg_psnr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d712c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf54c706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d57d44d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a459ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bd4b0915",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c=torch.tensor(a)\n",
    "array=[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b32edc16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0000555]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "73a48c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=1.0000555\n",
    "c=torch.tensor([a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6bfda5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "689a62f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1c008c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ebc1b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a20ae70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "17494bcf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got float)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [65], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected np.ndarray (got float)"
     ]
    }
   ],
   "source": [
    "torch.from_numpy(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02be585f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cbe20f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da9be60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e46792f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_items([('G_loss', 0.02739609405398369)])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "40aa9a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.030511628836393356\n",
      "tensor(0.0305)\n"
     ]
    }
   ],
   "source": [
    "for k,v in logs.items():\n",
    "    print(v)\n",
    "    a=torch.tensor(v)\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f8038858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_items([('G_loss', 0.02739609405398369)])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cbb188",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:swinir]",
   "language": "python",
   "name": "conda-env-swinir-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
